# default: Avg. LPIPS=0.0769, PSNR=27.4572, SSIM=0.8967
# FRB(skip): Avg. LPIPS=0.0878, PSNR=27.6970, SSIM=0.8998
# FULL(skip+bottleneck): Avg. LPIPS=0.0711, PSNR=27.7646, SSIM=0.8998
# FULL2(skip+bottleneck+PosEnc+Freqloss): Avg. LPIPS=0.0703, PSNR=27.8745, SSIM=0.8962
# FULL4(skip+bottleneckV2+PosEnc): Avg. LPIPS=0.0673, PSNR=28.2967, SSIM=0.9026
GENERAL:
  SEED: 123
  WORKER: 8
  SAVE_PREFIX: '.out_dir/tip'
  EXP_NAME: 'FULL5'

DATA:
  DATA_TYPE: TIP # Please specify the type of the dataset (select from AIM/UHDM/FHDMi/TIP)
  TRAIN_DATASET: './data_tip/trainData' # The training data path, e.g., './data_tip/trainData'
  TEST_DATASET: './data_tip/testData' # The test data path, e.g., './data_tip/testData'
  DEMO_DATASET: './data_tip/demoData' # The demo data path, e.g., './data_tip/demoData'

MODEL:
# ESDNet configuration
  # MODEL_NAME: 'ESDNet' # The model name, e.g., ESDNet, ESDNet-L
  # EN_FEATURE_NUM: 48 # The initial channel number of dense blocks of encoder
  # EN_INTER_NUM: 32 # The growth rate (intermediate channel number) of dense blocks of encoder
  # DE_FEATURE_NUM: 64 # The initial channel number of dense blocks of decoder
  # DE_INTER_NUM: 32 # The growth rate (intermediate channel number) of dense blocks of decoder
  # SAM_NUMBER: 1 # The number of SAM for each encoder or decoder level; set 1 for our ESDNet, and 2 for ESDNet-L
# UNet configuration
  # MODEL_NAME: 'UNet'
  # IN_DIM: 3
  # FEAT_DIM: 64
  # OUT_DIM: 3
# MyNet configuration
  MODEL_NAME: 'MyNet' # The model name, e.g., MyNet, MyNet-L
  CH_DIM: 3
  FEAT_DIM: 64
  

TRAIN:
  USE_GAN: True
  D_ITERS: 1
  BATCH_SIZE: 12
  #LOADER: crop # The loading way for training data, e.g., crop, resize, default; see ./dataset/load_data.py
  #CROP_SIZE: 768 # Set the crop size if LOADER==crop
  #RESIZE_SIZE: 384 # Set the resizing size if LOADER==crop
  SAVE_ITER: 500 # Save training images/results at each SAVE_ITER*n iter
  LOAD_EPOCH: False # If specify it, loading the corresponding model for resuming training
# VGGPerceptual configuration
  # LOSS_NAME: 'VGGPerceptual'
  # LAM: 1 # The loss weight for L1 loss
  # LAM_P: 1 # The loss weight for perceptual loss
# MyLoss configuration
  LOSS_NAME: 'MyLoss'
  LAM_P: 2
  LAM_ADV: 0.5

TEST:
  TEST_EPOCH: 'auto' # Input 'auto' for loading the latest model
  SAVE_IMG: False # The file type (e.g., jpg, png) for saving the output image; set False to avoid saving
  # LOAD_PATH: False # If specify a load path for a checkpoint, TEST_EPOCH will be deprecated
  LOAD_PATH: '.out_dir/tip/FULL5/net_checkpoints/checkpoint_latest.tar' # If specify a load path for a checkpoint, TEST_EPOCH will be deprecated
  EVALUATION_METRIC: True # If True, calculate metrics
  EVALUATION_TIME: False # If True, calculate processing time per image; EVALUATION_METRIC will be deprecated for accurate statistics
  EVALUATION_COST: False #If True, calculate MACs and Parameters number

SOLVER:
  EPOCHS: 70 # The total training epochs
  T_0: 10 # The total epochs for the first learning cycle (learning rate warms up then)
  T_MULT: 1 # The learning cycle would be (T_0, T_0*T_MULT, T_0*T_MULT^2, T_0*T_MULT^3, ...)
  ETA_MIN: 0.000001 # Initial learning rate in each learning cycle
  BASE_LR: 0.0002 # Learning rate in the end of each learning cycle

